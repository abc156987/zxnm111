[__init__.py](https://github.com/user-attachments/files/24687184/__init__.py)[sentiment_analysis.py](https://github.com/user-attachments/files/24687180/sentiment_analysis.py)[translator.py](https://github.com/user-attachments/files/24687176/translator.py)[text_classify.py](https://github.com/user-attachments/files/24687175/text_classify.py)ç¨‹çƒï¼ˆæœ¬äººï¼‰ï¼š
[10_3_1.py](https://github.com/user-attachments/files/24687135/10_3_1.py)[app.py](https://github.com/user-attachments/files/24687123/app.py)
[config.py](https://github.com/user-attachments/files/24687124/config.py)
[test_api.py](https://github.com/user-attachments/files/24687127/test_api.py)
[Up# æ–‡æœ¬åˆ†ç±»æ¨¡å‹åŠ è½½å’Œæ¨ç†
import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import load_model

# é…ç½®TensorFlowä½¿ç”¨CPUï¼Œé¿å…GPUç›¸å…³é”™è¯¯
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

# å¯¼å…¥é…ç½®
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import TEXT_CLASSIFY_MODEL_PATH, TEXT_CLASSIFY_VOCAB_PATH, TEXT_CLASSIFY_CATEGORIES, TEXT_CLASSIFY_SEQ_LENGTH

class TextClassifier:
    def __init__(self):
        self.model = None
        self.words = None
        self.word_to_id = None
        self.categories = TEXT_CLASSIFY_CATEGORIES
        self.seq_length = TEXT_CLASSIFY_SEQ_LENGTH
        self.load_model()
    
    def open_file(self, filename, mode='r'):
        """æ‰“å¼€æ–‡ä»¶"""
        return open(filename, mode, encoding='utf-8', errors='ignore')
    
    def read_vocab(self, vocab_dir):
        """è¯»å–è¯æ±‡è¡¨"""
        with self.open_file(vocab_dir) as fp:
            words = [i.strip() for i in fp.readlines()]
        word_to_id = dict(zip(words, range(len(words))))
        return words, word_to_id
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œè¯æ±‡è¡¨"""
        try:
            # è¯»å–è¯æ±‡è¡¨
            if os.path.exists(TEXT_CLASSIFY_VOCAB_PATH):
                self.words, self.word_to_id = self.read_vocab(TEXT_CLASSIFY_VOCAB_PATH)
            else:
                raise FileNotFoundError(f"è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {TEXT_CLASSIFY_VOCAB_PATH}")
            
            # ä½¿ç”¨CPUåŠ è½½æ¨¡å‹ï¼Œé¿å…GPUç›¸å…³é”™è¯¯
            with tf.device('/CPU:0'):
                # ä¼˜å…ˆåŠ è½½æœ€ä½³æ¨¡å‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åŠ è½½æœ€ç»ˆæ¨¡å‹
                best_model_path = TEXT_CLASSIFY_MODEL_PATH.replace('my_model.h5', 'best_model.h5')
                
                if os.path.exists(best_model_path):
                    self.model = load_model(best_model_path)
                    print(f"æ–‡æœ¬åˆ†ç±»æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆæœ€ä½³æ¨¡å‹ï¼‰: {best_model_path}")
                elif os.path.exists(TEXT_CLASSIFY_MODEL_PATH):
                    self.model = load_model(TEXT_CLASSIFY_MODEL_PATH)
                    print(f"æ–‡æœ¬åˆ†ç±»æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆæœ€ç»ˆæ¨¡å‹ï¼‰: {TEXT_CLASSIFY_MODEL_PATH}")
                else:
                    # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
                    alt_path = TEXT_CLASSIFY_MODEL_PATH.replace('my_model.h5', 'best_validation_best.h5')
                    if os.path.exists(alt_path):
                        self.model = load_model(alt_path)
                        print(f"æ–‡æœ¬åˆ†ç±»æ¨¡å‹åŠ è½½æˆåŠŸ: {alt_path}")
                    else:
                        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ã€‚å°è¯•è¿‡çš„è·¯å¾„: {best_model_path}, {TEXT_CLASSIFY_MODEL_PATH}")
        except Exception as e:
            print(f"åŠ è½½æ–‡æœ¬åˆ†ç±»æ¨¡å‹å¤±è´¥: {e}")
            self.model = None
    
    def preprocess_text(self, text):
        """é¢„å¤„ç†æ–‡æœ¬"""
        if not text:
            return None
        
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºå­—ç¬¦åˆ—è¡¨
        content = list(text)
        # è½¬æ¢ä¸ºIDåºåˆ—
        data_id = [self.word_to_id.get(x, 0) for x in content if x in self.word_to_id]
        
        if not data_id:
            return None
        
        # ä½¿ç”¨ pad_sequences å¡«å……åˆ°å›ºå®šé•¿åº¦ï¼ˆä¸è®­ç»ƒä»£ç ä¿æŒä¸€è‡´ï¼‰
        x_pad = keras.preprocessing.sequence.pad_sequences(
            [data_id], 
            maxlen=self.seq_length, 
            padding='post', 
            truncating='post'
        )
        return x_pad
    
    def predict(self, text):
        """é¢„æµ‹æ–‡æœ¬ç±»åˆ«"""
        if self.model is None:
            return {"error": "æ¨¡å‹æœªåŠ è½½"}
        
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            x_pad = self.preprocess_text(text)
            if x_pad is None:
                return {"error": "æ–‡æœ¬é¢„å¤„ç†å¤±è´¥"}
            
            # ä½¿ç”¨CPUè¿›è¡Œé¢„æµ‹ï¼Œé¿å…GPUç›¸å…³é”™è¯¯
            with tf.device('/CPU:0'):
                # é¢„æµ‹
                y_pred = self.model.predict(x_pad, verbose=0)
                predicted_class_idx = np.argmax(y_pred[0])
                confidence = float(y_pred[0][predicted_class_idx])
                predicted_class = self.categories[predicted_class_idx]
                
                # è¿”å›æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
                probabilities = {
                    self.categories[i]: float(y_pred[0][i]) 
                    for i in range(len(self.categories))
                }
                
                return {
                    "category": predicted_class,
                    "confidence": confidence,
                    "probabilities": probabilities
                }
        except Exception as e:
            error_msg = str(e)
            # å¦‚æœæ˜¯GPUç›¸å…³é”™è¯¯ï¼Œæä¾›æ›´å‹å¥½çš„æç¤º
            if "stream" in error_msg.lower() or "gpu" in error_msg.lower():
                return {"error": "æ¨¡å‹é¢„æµ‹æ—¶å‘ç”Ÿè®¾å¤‡é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•"}
            return {"error": f"é¢„æµ‹å¤±è´¥: {error_msg}"}

# å…¨å±€å®ä¾‹
text_classifier = TextClassifier()

loading text_classify.pyâ€¦]()

[Upload# 10.3.1 æ–‡æœ¬åˆ†ç±»
# ä»£ç 10-1 è‡ªå®šä¹‰è¯­æ–™é¢„å¤„ç†å‡½æ•°
import tensorflow as tf
from collections import Counter
from tensorflow import keras
import numpy as np
import seaborn as sns
from keras.models import load_model
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
from matplotlib.pyplot import MultipleLocator
import os
# æ‰“å¼€æ–‡ä»¶
def open_file(filename, mode='r'):
    '''
    filenameï¼šè¡¨ç¤ºè¯»å–/å†™å…¥çš„æ–‡ä»¶è·¯å¾„
    modeï¼š'r' or 'w'è¡¨ç¤ºè¯»å–/å†™å…¥æ–‡ä»¶
    '''
    return open(filename, mode, encoding='utf-8', errors='ignore')
# è¯»å–æ–‡ä»¶æ•°æ®
def read_file(filename):
    '''
    filenameï¼šè¡¨ç¤ºæ–‡ä»¶è·¯å¾„
    '''
    contents, labels = [], []
    with open_file(filename) as f:
        for line in f:
            try:
                label, content = line.strip().split('\t')  # æŒ‰ç…§åˆ¶è¡¨ç¬¦åˆ†å‰²å­—ç¬¦ä¸²
                if content:
                    contents.append(list(content))
                    labels.append(label)
            except:
                pass
    return contents, labels
# æ„å»ºè¯æ±‡è¡¨
def build_vocab(train_dir, vocab_dir, vocab_size=5000):
    '''
    train_dirï¼šè®­ç»ƒé›†æ–‡ä»¶çš„å­˜æ”¾è·¯å¾„
    vocab_dirï¼šè¯æ±‡è¡¨çš„å­˜æ”¾è·¯å¾„
    vocab_sizeï¼šè¯æ±‡è¡¨çš„å¤§å°
    '''
    data_train, lab = read_file(train_dir)
    all_data = []
    for content in data_train:
        all_data.extend(content)
    counter = Counter(all_data)  # è¯è¢‹
    count_pairs = counter.most_common(vocab_size - 1)  # top n
    words, temp = list(zip(*count_pairs))  # è·å–key
    words = ['<PAD>'] + list(words)  # æ·»åŠ ä¸€ä¸ª<PAD>å°†æ‰€æœ‰æ–‡æœ¬padä¸ºåŒä¸€é•¿åº¦
    open_file(vocab_dir, mode='w').write('\n'.join(words) + '\n')
# è¯»å–è¯æ±‡è¡¨
def read_vocab(vocab_dir):
    '''
    vocab_dirï¼šè¯æ±‡è¡¨çš„å­˜æ”¾è·¯å¾„
    '''
    with open_file(vocab_dir) as fp:
        words = [i.strip() for i in fp.readlines()]
    word_to_id = dict(zip(words, range(len(words))))
    return words, word_to_id
# è¯»å–åˆ†ç±»ç›®å½•
def read_category():
    categories = ['ä½“è‚²', 'è´¢ç»', 'æˆ¿äº§', 'å®¶å±…', 'æ•™è‚²', 'ç§‘æŠ€', 'æ—¶å°š', 'æ—¶æ”¿', 'æ¸¸æˆ', 'å¨±ä¹']
    # å¾—åˆ°ç±»åˆ«ä¸ç¼–å·ç›¸å¯¹åº”çš„å­—å…¸ï¼Œåˆ†åˆ«ä¸º0-9
    cat_to_id = dict(zip(categories, range(len(categories))))
    return categories, cat_to_id
# å°†idè¡¨ç¤ºçš„å†…å®¹è½¬æ¢ä¸ºæ–‡å­—
def to_words(content, words):
    '''
    contentï¼šidè¡¨ç¤ºçš„å†…å®¹
    wordsï¼šæ–‡æœ¬å†…å®¹
    '''
    return ''.join(words[x] for x in content)
# å°†æ–‡ä»¶è½¬æ¢ä¸ºidè¡¨ç¤º
def process_file(filename, word_to_id, cat_to_id, max_length=600):
    '''
    filenameï¼šæ–‡ä»¶è·¯å¾„
    word_to_idï¼šè¯æ±‡è¡¨
    cat_to_idï¼šç±»åˆ«å¯¹åº”çš„ç¼–å·
    max_lengthï¼šè¯å‘é‡çš„æœ€å¤§é•¿åº¦
    '''
    contents, labels = read_file(filename)
    data_id, label_id = [], []
    for i in range(len(contents)):
        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])
        label_id.append(cat_to_id[labels[i]])
    # ä½¿ç”¨Kerasæä¾›çš„pad_sequenceså°†æ–‡æœ¬padä¸ºå›ºå®šé•¿åº¦
    x_pad = keras.preprocessing.sequence.pad_sequences(data_id, max_length)
    # å°†æ ‡ç­¾è½¬ä¸ºç‹¬çƒ­ç¼–ç ï¼ˆone-hotï¼‰è¡¨ç¤º
    y_pad = keras.utils.to_categorical(label_id, num_classes=len(cat_to_id))
    return x_pad, y_pad


# ä»£ç 10-2 åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†

# è®¾ç½®æ•°æ®è¯»å–ã€æ¨¡å‹ã€ç»“æœä¿å­˜è·¯å¾„
base_dir = '/root/autodl-tmp/NLP/nlp_deeplearn/data/'
train_dir = os.path.join(base_dir, 'cnews.train.txt')
test_dir = os.path.join(base_dir, 'cnews.test.txt')
val_dir = os.path.join(base_dir, 'cnews.val.txt')
vocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')
save_dir = '/root/autodl-tmp/NLP/nlp_deeplearn/tmp/'
save_path = os.path.join(save_dir, 'best_validation')

# è‹¥ä¸å­˜åœ¨è¯æ±‡è¡¨ï¼Œåˆ™é‡æ–°å»ºç«‹è¯æ±‡è¡¨
vocab_size = 5000
if not os.path.exists(vocab_dir):
    build_vocab(train_dir, vocab_dir, vocab_size)

# è¯»å–åˆ†ç±»ç›®å½•
categories, cat_to_id = read_category()
# è¯»å–è¯æ±‡è¡¨
words, word_to_id = read_vocab(vocab_dir)
# è¯æ±‡è¡¨å¤§å°
vocab_size = len(words)

# æ•°æ®åŠ è½½
seq_length = 600  # åºåˆ—é•¿åº¦

# è·å–è®­ç»ƒæ•°æ®
x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, seq_length)
# è·å–éªŒè¯æ•°æ®
x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, seq_length)
# è·å–æµ‹è¯•æ•°æ®
x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, seq_length)

# ä»£ç 10-3 è®¾ç½®æ¨¡å‹å‚æ•°å¹¶æ„å»ºæ¨¡å‹


# æ­å»ºç®€åŒ–çš„LSTMæ¨¡å‹ï¼ˆå•å±‚åŒå‘LSTMï¼‰
def TextRNN():
    model = tf.keras.Sequential()
    # åµŒå…¥å±‚ï¼ˆé™ä½ç»´åº¦ä»¥åŠ å¿«è®­ç»ƒï¼‰
    model.add(tf.keras.layers.Embedding(vocab_size+1, 128, input_length=600, mask_zero=True))
    model.add(tf.keras.layers.Dropout(0.2))
    
    # å•å±‚åŒå‘LSTMï¼ˆç®€åŒ–ç»“æ„ï¼‰
    model.add(tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)
    ))
    
    # ç®€åŒ–çš„å…¨è¿æ¥å±‚
    model.add(tf.keras.layers.Dense(128, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.3))
    
    model.add(tf.keras.layers.Dense(64, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.2))
    
    # è¾“å‡ºå±‚
    model.add(tf.keras.layers.Dense(10, activation='softmax'))
    return model

# ä»£ç 10-4 æ¨¡å‹è®­ç»ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰

# ä½¿ç”¨å›è°ƒå‡½æ•°ä¿å­˜æœ€ä½³æ¨¡å‹
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

# åˆ›å»ºä¿å­˜ç›®å½•
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# ç®€åŒ–çš„å›è°ƒå‡½æ•°
callbacks = [
    # ä¿å­˜æœ€ä½³éªŒè¯å‡†ç¡®ç‡çš„æ¨¡å‹
    ModelCheckpoint(
        filepath=os.path.join(save_dir, 'best_model.h5'),
        monitor='val_categorical_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    ),
    # æ—©åœæœºåˆ¶ï¼ˆå‡å°‘patienceä»¥åŠ å¿«è®­ç»ƒï¼‰
    EarlyStopping(
        monitor='val_categorical_accuracy',
        patience=3,
        restore_best_weights=True,
        verbose=1
    )
]

# è®­ç»ƒå‚æ•°è®¾ç½®ï¼ˆä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œæ€§èƒ½æ›´å¥½ï¼‰
try:
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    with strategy.scope():
        model = TextRNN()
        # ä½¿ç”¨Adamä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡è¡°å‡
        model.compile(
            loss='categorical_crossentropy',
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            metrics=['categorical_accuracy']
        )
except:
    # å¦‚æœå¤šGPUç­–ç•¥å¤±è´¥ï¼Œä½¿ç”¨å•GPUæˆ–CPU
    model = TextRNN()
    model.compile(
        loss='categorical_crossentropy',
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        metrics=['categorical_accuracy']
    )

# æ¨¡å‹è®­ç»ƒï¼ˆç®€åŒ–è®­ç»ƒè½®æ¬¡ï¼‰
history = model.fit(
    x_train, y_train, 
    batch_size=128,  # å¢å¤§batch sizeä»¥åŠ å¿«è®­ç»ƒ
    epochs=10,  # å‡å°‘è®­ç»ƒè½®æ•°
    validation_data=(x_val, y_val),
    callbacks=callbacks,
    verbose=1
)
# è®¾ç½®ç»˜å›¾çš„å­—ä½“
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['SIMHEI']
# ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹
def plot_acc_loss(history):
    '''
    historyï¼šæ¨¡å‹è®­ç»ƒçš„è¿”å›å€¼
    '''
    plt.subplot(121)
    plt.title('å‡†ç¡®ç‡è¶‹åŠ¿å›¾')
    epochs_trained = len(history.history['categorical_accuracy'])
    plt.plot(range(1, epochs_trained+1), history.history['categorical_accuracy'], linestyle='-', color='g', label='è®­ç»ƒé›†')
    plt.plot(range(1, epochs_trained+1), history.history['val_categorical_accuracy'], linestyle='-.', color='b', label='éªŒè¯é›†')
    plt.legend(loc='best')  # è®¾ç½®å›¾ä¾‹
    # xè½´æŒ‰1åˆ»åº¦æ˜¾ç¤º
    x_major_locator = MultipleLocator(1)
    ax = plt.gca()
    ax.xaxis.set_major_locator(x_major_locator)  
    plt.tick_params(axis='both', which='major', labelsize=7)
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('å‡†ç¡®ç‡')
    plt.subplot(122)
    plt.title('æŸå¤±è¶‹åŠ¿å›¾')
    epochs_trained = len(history.history['loss'])
    plt.plot(range(1, epochs_trained+1), history.history['loss'], linestyle='-', color='g', label='è®­ç»ƒé›†')
    plt.plot(range(1, epochs_trained+1), history.history['val_loss'], linestyle='-.', color='b', label='éªŒè¯é›†')
    plt.legend(loc='best')
    # xè½´æŒ‰1åˆ»åº¦æ˜¾ç¤º
    x_major_locator = MultipleLocator(1)
    ax = plt.gca()
    ax.xaxis.set_major_locator(x_major_locator)  
    plt.tick_params(axis='both', which='major', labelsize=7)
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('æŸå¤±å€¼')
    plt.tight_layout()
    plt.show()
    plt.savefig("3.png")
plot_acc_loss(history)

# ä»£ç 10-5 æŸ¥çœ‹æ¨¡å‹æ¶æ„å¹¶ä¿å­˜æ¨¡å‹
model.summary()
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
final_model_path = os.path.join(save_dir, 'my_model.h5')
model.save(final_model_path)
print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")

# å¦‚æœå­˜åœ¨æœ€ä½³æ¨¡å‹ï¼Œä¹ŸåŠ è½½å®ƒç”¨äºæµ‹è¯•
best_model_path = os.path.join(save_dir, 'best_model.h5')
if os.path.exists(best_model_path):
    print(f"ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•: {best_model_path}")
    model1 = load_model(best_model_path)
else:
    print(f"ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œæµ‹è¯•: {final_model_path}")
    model1 = model

# ä»£ç 10-6 æ¨¡å‹æµ‹è¯•

# å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
y_pre = model1.predict(x_test)
# è®¡ç®—æ··æ·†çŸ©é˜µ
confm = confusion_matrix(np.argmax(y_pre, axis=1), np.argmax(y_test, axis=1))
# æ‰“å°æ¨¡å‹è¯„ä»·
print(classification_report(np.argmax(y_pre, axis=1), np.argmax(y_test, axis=1)))

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
plt.figure(figsize=(8, 8), dpi=600)
# è®¾ç½®ç»˜å›¾çš„å­—ä½“
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['SIMHEI']
sns.heatmap(confm.T, square=True, annot=True,
            fmt='d', cbar=False, linewidths=.8,
            cmap='YlGnBu')
plt.xlabel('çœŸå®æ ‡ç­¾', size=14)
plt.ylabel('é¢„æµ‹æ ‡ç­¾', size=14)
plt.xticks(np.arange(10)+0.5, categories, size=12)
plt.yticks(np.arange(10)+0.3, categories, size=12)
plt.show()
plt.savefig("1.png")ing 10_3_1.pyâ€¦]()
æä½³éŸ³ï¼š
[10_3_2.py](https://github.com/user-attachments/files/24687156/10_3_2.py)
[Uploading senti# æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å’Œæ¨ç†
import os
import re
import numpy as np
import pandas as pd
import jieba
import pickle
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import sequence
import sys

# é…ç½®TensorFlowä½¿ç”¨CPUï¼Œé¿å…GPUç›¸å…³é”™è¯¯
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import SENTIMENT_MODEL_PATH, SENTIMENT_DICT_PATH, SENTIMENT_SEQ_LENGTH

class SentimentAnalyzer:
    def __init__(self):
        self.model = None
        self.dicts = None
        self.maxlen = SENTIMENT_SEQ_LENGTH
        self.confidence_threshold = 0.5  # ç½®ä¿¡åº¦é˜ˆå€¼
        self._init_keywords()
        self.load_model()
    
    def _init_keywords(self):
        """åˆå§‹åŒ–æƒ…æ„Ÿå…³é”®è¯è¯å…¸"""
        # æ­£é¢æƒ…æ„Ÿè¯ï¼ˆæ›´å…¨é¢ï¼‰
        self.positive_words = [
            'å¥½', 'æ£’', 'èµ', 'å–œæ¬¢', 'æ»¡æ„', 'ä¸é”™', 'ä¼˜ç§€', 'å®Œç¾', 'å¼€å¿ƒ', 'é«˜å…´',
            'çˆ±', 'ç¾', 'æ£’æäº†', 'å¤ªå¥½äº†', 'æ¨è', 'å€¼å¾—', 'æ»¡æ„', 'èµ', 'ğŸ‘',
            'å–œæ¬¢', 'å–œçˆ±', 'çƒ­çˆ±', 'èµç¾', 'ç§°èµ', 'è¡¨æ‰¬', 'å¤¸å¥–', 'æ¬£èµ', 'è®¤å¯',
            'æ”¯æŒ', 'èµåŒ', 'åŒæ„', 'è‚¯å®š', 'æ­£é¢', 'ç§¯æ', 'ä¹è§‚', 'æ„‰å¿«', 'æ¬¢ä¹',
            'å…´å¥‹', 'æ¿€åŠ¨', 'æƒŠå–œ', 'æ„ŸåŠ¨', 'æ¸©æš–', 'èˆ’é€‚', 'å®‰å¿ƒ', 'æ”¾å¿ƒ', 'ä¿¡ä»»',
            'æˆåŠŸ', 'èƒœåˆ©', 'æˆå°±', 'è¿›æ­¥', 'æå‡', 'æ”¹å–„', 'ä¼˜åŒ–', 'å¢å¼º', 'åŠ å¼º',
            'ç¾å¥½', 'ç²¾å½©', 'å‡ºè‰²', 'å“è¶Š', 'æ°å‡º', 'ä¼˜ç§€', 'ä¼˜è‰¯', 'ä¼˜è´¨', 'ä¸Šä¹˜',
            'è¶…å€¼', 'åˆ’ç®—', 'å®æƒ ', 'ä¾¿å®œ', 'ç»æµ', 'é«˜æ•ˆ', 'å¿«é€Ÿ', 'ä¾¿æ·', 'æ–¹ä¾¿'
        ]
        
        # è´Ÿé¢æƒ…æ„Ÿè¯ï¼ˆæ›´å…¨é¢ï¼‰
        self.negative_words = [
            'å·®', 'å', 'çƒ‚', 'è®¨åŒ', 'å¤±æœ›', 'ç³Ÿç³•', 'åƒåœ¾', 'ä¸å¥½', 'ä¼¤å¿ƒ', 'éš¾è¿‡',
            'å·®åŠ²', 'ä¸è¡Œ', 'ä¸æ¨è', 'åæ‚”', 'ç³Ÿç³•', 'å·®è¯„', 'ğŸ‘',
            'è®¨åŒ', 'åŒæ¶', 'åæ„Ÿ', 'å«Œå¼ƒ', 'é„™è§†', 'æ‰¹è¯„', 'æŒ‡è´£', 'æŠ±æ€¨', 'åŸ‹æ€¨',
            'åå¯¹', 'æ‹’ç»', 'å¦å®š', 'è´Ÿé¢', 'æ¶ˆæ', 'æ‚²è§‚', 'æ²®ä¸§', 'å¤±è½', 'ç»æœ›',
            'æ„¤æ€’', 'ç”Ÿæ°”', 'æ¼ç«', 'çƒ¦èº', 'ç„¦è™‘', 'æ‹…å¿ƒ', 'å¿§è™‘', 'ææƒ§', 'å®³æ€•',
            'å¤±è´¥', 'æŒ«æŠ˜', 'å›°éš¾', 'é—®é¢˜', 'éº»çƒ¦', 'å›°æ‰°', 'é˜»ç¢', 'éšœç¢', 'ç¼ºé™·',
            'ç³Ÿç³•', 'æ¶åŠ£', 'ä½åŠ£', 'åŠ£è´¨', 'æ¬¡å“', 'æ®‹æ¬¡', 'ç ´æŸ', 'æŸå', 'æ•…éšœ',
            'æ˜‚è´µ', 'æµªè´¹', 'ä½æ•ˆ', 'ç¼“æ…¢', 'éº»çƒ¦', 'å¤æ‚', 'å›°éš¾', 'ä¸ä¾¿', 'ä¸å®ç”¨'
        ]
        
        # å¦å®šè¯
        self.negation_words = ['ä¸', 'æ²¡', 'æ— ', 'é', 'æœª', 'åˆ«', 'è«', 'å‹¿', 'å¦', 'æ²¡æœ‰', 'ä¸æ˜¯', 'ä¸èƒ½', 'ä¸ä¼š', 'ä¸æƒ³', 'ä¸è¦']
        
        # ç¨‹åº¦è¯ï¼ˆå¢å¼ºæƒ…æ„Ÿå¼ºåº¦ï¼‰
        self.intensity_words = {
            'éå¸¸': 1.5, 'ç‰¹åˆ«': 1.5, 'æå…¶': 1.8, 'ååˆ†': 1.4, 'ç›¸å½“': 1.3,
            'å¾ˆ': 1.2, 'æŒº': 1.1, 'æ¯”è¾ƒ': 0.9, 'æœ‰ç‚¹': 0.7, 'ç¨å¾®': 0.6,
            'è¶…çº§': 1.6, 'è¶…': 1.5, 'å¤ª': 1.4, 'æœ€': 1.7, 'æ›´': 1.2,
            'æå…¶': 1.8, 'æåº¦': 1.7, 'å¼‚å¸¸': 1.5, 'æ ¼å¤–': 1.4
        }
        
        # åœç”¨è¯ï¼ˆç”¨äºæ–‡æœ¬æ¸…æ´—ï¼Œæ³¨æ„ï¼šä¸åŒ…å«å¦å®šè¯ï¼‰
        self.stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'çœ‹',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'ä¸ª', 'ä¸­', 'ä¸º', 'è€Œ',
            'ä¸', 'åŠ', 'æˆ–', 'ä½†', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥', 'è™½ç„¶', 'ç„¶è€Œ'
        }
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å’Œè¯å…¸"""
        try:
            # ä½¿ç”¨CPUåŠ è½½æ¨¡å‹ï¼Œé¿å…GPUç›¸å…³é”™è¯¯
            with tf.device('/CPU:0'):
                # åŠ è½½æ¨¡å‹
                if os.path.exists(SENTIMENT_MODEL_PATH):
                    self.model = load_model(SENTIMENT_MODEL_PATH)
                    print(f"æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ: {SENTIMENT_MODEL_PATH}")
                else:
                    print(f"è­¦å‘Š: æƒ…æ„Ÿåˆ†ææ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {SENTIMENT_MODEL_PATH}")
                    self.model = None
            
            # åŠ è½½æˆ–åˆ›å»ºè¯å…¸
            if os.path.exists(SENTIMENT_DICT_PATH):
                with open(SENTIMENT_DICT_PATH, 'rb') as f:
                    self.dicts = pickle.load(f)
                print(f"æƒ…æ„Ÿåˆ†æè¯å…¸åŠ è½½æˆåŠŸ: {SENTIMENT_DICT_PATH}")
            else:
                print(f"è­¦å‘Š: æƒ…æ„Ÿåˆ†æè¯å…¸æ–‡ä»¶ä¸å­˜åœ¨: {SENTIMENT_DICT_PATH}")
                print("å°†ä½¿ç”¨ç®€åŒ–ç‰ˆæƒ…æ„Ÿåˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰")
                self.dicts = None
        except Exception as e:
            print(f"åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹å¤±è´¥: {e}")
            self.model = None
            self.dicts = None
    
    def clean_text(self, text):
        """æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ã€URLã€æ•°å­—ç­‰"""
        if not text:
            return ""
        
        # å»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        # å»é™¤é‚®ç®±
        text = re.sub(r'\S+@\S+', '', text)
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # å»é™¤ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š]', '', text)
        # å»é™¤çº¯æ•°å­—
        text = re.sub(r'\d+', '', text)
        
        return text.strip()
    
    def preprocess_text(self, text):
        """é¢„å¤„ç†æ–‡æœ¬"""
        if not text:
            return None
        
        try:
            # æ¸…æ´—æ–‡æœ¬
            cleaned_text = self.clean_text(text)
            if not cleaned_text:
                return None
            
            # åˆ†è¯
            words = list(jieba.cut(cleaned_text))
            
            # å»é™¤åœç”¨è¯å’Œç©ºå­—ç¬¦
            words = [w for w in words if w.strip() and w not in self.stop_words and len(w.strip()) > 0]
            
            if not words:
                return None
            
            if self.dicts is not None:
                # ä½¿ç”¨è®­ç»ƒæ—¶çš„è¯å…¸
                word_ids = []
                for word in words:
                    if word in self.dicts.index:
                        word_ids.append(self.dicts.loc[word, 'id'])
                
                if not word_ids:
                    return None
                
                # å¡«å……åºåˆ—
                sent = sequence.pad_sequences([word_ids], maxlen=self.maxlen)
                return sent
            else:
                # ç®€åŒ–ç‰ˆï¼šåŸºäºå…³é”®è¯çš„æƒ…æ„Ÿåˆ†æ
                return None
        except Exception as e:
            print(f"æ–‡æœ¬é¢„å¤„ç†é”™è¯¯: {e}")
            return None
    
    def predict_with_keywords(self, text):
        """åŸºäºå…³é”®è¯çš„ç®€åŒ–æƒ…æ„Ÿåˆ†æï¼ˆè€ƒè™‘å¦å®šè¯å’Œç¨‹åº¦è¯ï¼‰"""
        if not text:
            return {"sentiment": "ä¸­æ€§", "confidence": 0.5}
        
        # æ¸…æ´—æ–‡æœ¬
        cleaned_text = self.clean_text(text)
        if not cleaned_text:
            return {"sentiment": "ä¸­æ€§", "confidence": 0.5}
        
        # åˆ†è¯
        words = list(jieba.cut(cleaned_text))
        words = [w for w in words if w.strip() and w not in self.stop_words]
        
        if not words:
            return {"sentiment": "ä¸­æ€§", "confidence": 0.5}
        
        pos_score = 0.0
        neg_score = 0.0
        
        # éå†æ¯ä¸ªè¯ï¼Œè€ƒè™‘å¦å®šè¯å’Œç¨‹åº¦è¯çš„å½±å“
        for i, word in enumerate(words):
            intensity = 1.0  # é»˜è®¤å¼ºåº¦
            negated = False  # æ˜¯å¦è¢«å¦å®š
            
            # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰ç¨‹åº¦è¯ï¼ˆæ£€æŸ¥å‰1-2ä¸ªè¯ï¼‰
            for j in range(max(0, i-2), i):
                if words[j] in self.intensity_words:
                    intensity = self.intensity_words[words[j]]
                    break
            
            # æ£€æŸ¥å‰é¢æ˜¯å¦æœ‰å¦å®šè¯ï¼ˆæ£€æŸ¥å‰1-3ä¸ªè¯ï¼Œå› ä¸ºå¦å®šè¯å¯èƒ½è·ç¦»è¾ƒè¿œï¼‰
            for j in range(max(0, i-3), i):
                if words[j] in self.negation_words:
                    negated = True
                    break
            
            # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
            if word in self.positive_words:
                score = 1.0 * intensity
                if negated:
                    neg_score += score  # å¦å®šæ­£é¢è¯ = è´Ÿé¢
                else:
                    pos_score += score
            
            elif word in self.negative_words:
                score = 1.0 * intensity
                if negated:
                    pos_score += score  # å¦å®šè´Ÿé¢è¯ = æ­£é¢
                else:
                    neg_score += score
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æƒ…æ„Ÿè¯ï¼Œè¿”å›ä¸­æ€§
        total_score = pos_score + neg_score
        if total_score == 0:
            return {"sentiment": "ä¸­æ€§", "confidence": 0.5}
        
        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºåˆ†æ•°å·®å¼‚å’Œæ€»åˆ†æ•°ï¼‰
        score_diff = abs(pos_score - neg_score)
        # å¦‚æœåˆ†æ•°å·®å¼‚æ˜æ˜¾ï¼Œç½®ä¿¡åº¦æ›´é«˜
        if total_score > 0:
            confidence = 0.5 + min(score_diff / total_score * 0.45, 0.45)
        else:
            confidence = 0.5
        
        # åˆ¤æ–­æƒ…æ„Ÿï¼ˆæ”¹è¿›åˆ¤æ–­é€»è¾‘ï¼Œé™ä½é˜ˆå€¼ä»¥æé«˜å‡†ç¡®æ€§ï¼‰
        # å¦‚æœè´Ÿé¢åˆ†æ•°æ˜æ˜¾å¤§äºæ­£é¢åˆ†æ•°ï¼Œåˆ¤å®šä¸ºè´Ÿé¢
        if neg_score > pos_score * 1.2:  # è´Ÿé¢åˆ†æ•°è‡³å°‘æ˜¯æ­£é¢çš„1.2å€
            return {"sentiment": "è´Ÿé¢", "confidence": min(confidence, 0.95)}
        elif pos_score > neg_score * 1.2:  # æ­£é¢åˆ†æ•°è‡³å°‘æ˜¯è´Ÿé¢çš„1.2å€
            return {"sentiment": "æ­£é¢", "confidence": min(confidence, 0.95)}
        elif neg_score > 0 and pos_score == 0:
            # åªæœ‰è´Ÿé¢è¯ï¼Œæ²¡æœ‰æ­£é¢è¯
            return {"sentiment": "è´Ÿé¢", "confidence": min(confidence, 0.9)}
        elif pos_score > 0 and neg_score == 0:
            # åªæœ‰æ­£é¢è¯ï¼Œæ²¡æœ‰è´Ÿé¢è¯
            return {"sentiment": "æ­£é¢", "confidence": min(confidence, 0.9)}
        else:
            # æ­£é¢å’Œè´Ÿé¢è¯éƒ½å­˜åœ¨ï¼Œæ ¹æ®æ¯”ä¾‹åˆ¤æ–­
            if neg_score > pos_score:
                return {"sentiment": "è´Ÿé¢", "confidence": min(confidence, 0.85)}
            elif pos_score > neg_score:
                return {"sentiment": "æ­£é¢", "confidence": min(confidence, 0.85)}
            else:
                return {"sentiment": "ä¸­æ€§", "confidence": 0.5}
    
    def predict(self, text):
        """é¢„æµ‹æ–‡æœ¬æƒ…æ„Ÿ"""
        if not text or not text.strip():
            return {"sentiment": "ä¸­æ€§", "confidence": 0.5, "method": "default"}
        
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨å…³é”®è¯æ–¹æ³•
        if self.model is None:
            result = self.predict_with_keywords(text)
            result["method"] = "keywords"
            return result
    
        try:
            # é¢„å¤„ç†æ–‡æœ¬
            x_pad = self.preprocess_text(text)
            if x_pad is None:
                # å¦‚æœé¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯æ–¹æ³•
                result = self.predict_with_keywords(text)
                result["method"] = "keywords_fallback"
                return result
        
            # ä½¿ç”¨æ¨¡å‹é¢„æµ‹
            with tf.device('/CPU:0'):
                y_pred = self.model.predict(x_pad, verbose=0)
        
            # å¤„ç†æ¨¡å‹è¾“å‡ºï¼ˆæ ¹æ®è®­ç»ƒä»£ç ï¼Œæ¨¡å‹ä½¿ç”¨ sigmoid è¾“å‡ºï¼Œæ ‡ç­¾ï¼š1=æ­£é¢ï¼Œ0=è´Ÿé¢ï¼‰
            # æ¨¡å‹è¾“å‡ºå½¢çŠ¶å¯èƒ½æ˜¯ (1, 1) æˆ– (1,)
            if len(y_pred.shape) == 2 and y_pred.shape[1] == 2:
                # äºŒåˆ†ç±» softmax è¾“å‡ºï¼ˆå¦‚æœæ¨¡å‹è¢«ä¿®æ”¹è¿‡ï¼‰
                negative_prob = float(y_pred[0][0])  # ç¬¬ä¸€ä¸ªç±»åˆ«ï¼ˆè´Ÿé¢=0ï¼‰
                positive_prob = float(y_pred[0][1])  # ç¬¬äºŒä¸ªç±»åˆ«ï¼ˆæ­£é¢=1ï¼‰
                
                # åˆ¤æ–­æƒ…æ„Ÿ
                if positive_prob > negative_prob:
                    sentiment = "æ­£é¢"
                    confidence = positive_prob
                else:
                    sentiment = "è´Ÿé¢"
                    confidence = negative_prob
                
                # è·å–å…³é”®è¯é¢„æµ‹ç»“æœç”¨äºéªŒè¯
                keyword_result = self.predict_with_keywords(text)
                
                # å¦‚æœæ¨¡å‹ç½®ä¿¡åº¦è¾ƒä½ï¼Œæˆ–è€…æ¨¡å‹é¢„æµ‹ä¸å…³é”®è¯é¢„æµ‹ä¸ä¸€è‡´ï¼Œéœ€è¦è°¨æ…å¤„ç†
                model_uncertain = confidence < self.confidence_threshold or abs(positive_prob - negative_prob) < 0.15
                prediction_conflict = sentiment != keyword_result["sentiment"] and keyword_result["sentiment"] != "ä¸­æ€§"
                
                if model_uncertain or prediction_conflict:
                    # å½“æ¨¡å‹ä¸ç¡®å®šæˆ–ä¸å…³é”®è¯é¢„æµ‹å†²çªæ—¶ï¼Œä¼˜å…ˆå‚è€ƒå…³é”®è¯ç»“æœ
                    if prediction_conflict and keyword_result["confidence"] > 0.7:
                        # å¦‚æœå…³é”®è¯é¢„æµ‹ç½®ä¿¡åº¦é«˜ä¸”ä¸æ¨¡å‹å†²çªï¼Œä¼˜å…ˆä½¿ç”¨å…³é”®è¯ç»“æœ
                        sentiment = keyword_result["sentiment"]
                        # é™ä½æ¨¡å‹æƒé‡ï¼Œæé«˜å…³é”®è¯æƒé‡
                        combined_confidence = (confidence * 0.3 + keyword_result["confidence"] * 0.7)
                        confidence = combined_confidence
                        return {
                            "sentiment": sentiment,
                            "confidence": float(confidence),
                            "negative_prob": negative_prob,
                            "positive_prob": positive_prob,
                            "method": "model_keywords_combined",
                            "model_sentiment": "æ­£é¢" if positive_prob > negative_prob else "è´Ÿé¢",
                            "keyword_sentiment": keyword_result["sentiment"]
                        }
                    else:
                        # æ¨¡å‹ä¸ç¡®å®šä½†æ— å†²çªï¼Œæˆ–å…³é”®è¯ä¹Ÿä¸ç¡®å®šï¼Œä½¿ç”¨åŠ æƒå¹³å‡
                        combined_confidence = (confidence * 0.4 + keyword_result["confidence"] * 0.6)
                        if abs(positive_prob - negative_prob) < 0.1:  # æ¦‚ç‡æ¥è¿‘æ—¶ï¼Œå‚è€ƒå…³é”®è¯ç»“æœ
                            sentiment = keyword_result["sentiment"]
                        confidence = combined_confidence
                
                return {
                    "sentiment": sentiment,
                    "confidence": float(confidence),
                    "negative_prob": negative_prob,
                    "positive_prob": positive_prob,
                    "method": "model"
                }
            else:
                # å¤„ç† sigmoid å•å€¼è¾“å‡º
                # è¾“å‡ºå½¢çŠ¶å¯èƒ½æ˜¯ (1, 1) æˆ– (1,)
                if len(y_pred.shape) == 2:
                    sentiment_score = float(y_pred[0][0])  # å½¢çŠ¶ä¸º (1, 1)
                else:
                    sentiment_score = float(y_pred[0])  # å½¢çŠ¶ä¸º (1,)
            
            # å¤„ç† sigmoid è¾“å‡ºï¼ˆæ ¹æ®è®­ç»ƒä»£ç ï¼š1=æ­£é¢ï¼Œ0=è´Ÿé¢ï¼‰
            # sentiment_score æ¥è¿‘ 1 è¡¨ç¤ºæ­£é¢ï¼Œæ¥è¿‘ 0 è¡¨ç¤ºè´Ÿé¢
            if sentiment_score >= 0.5:
                sentiment = "æ­£é¢"
                confidence = sentiment_score
            else:
                sentiment = "è´Ÿé¢"
                confidence = 1 - sentiment_score
            
            # è·å–å…³é”®è¯é¢„æµ‹ç»“æœç”¨äºéªŒè¯
            keyword_result = self.predict_with_keywords(text)
            
            # å¦‚æœæ¨¡å‹ç½®ä¿¡åº¦è¾ƒä½ï¼Œæˆ–è€…æ¨¡å‹é¢„æµ‹ä¸å…³é”®è¯é¢„æµ‹ä¸ä¸€è‡´ï¼Œéœ€è¦è°¨æ…å¤„ç†
            model_uncertain = confidence < self.confidence_threshold or abs(sentiment_score - 0.5) < 0.15
            prediction_conflict = sentiment != keyword_result["sentiment"] and keyword_result["sentiment"] != "ä¸­æ€§"
            
            if model_uncertain or prediction_conflict:
                # å½“æ¨¡å‹ä¸ç¡®å®šæˆ–ä¸å…³é”®è¯é¢„æµ‹å†²çªæ—¶ï¼Œä¼˜å…ˆå‚è€ƒå…³é”®è¯ç»“æœ
                # ç‰¹åˆ«æ˜¯å¯¹äºæ˜æ˜¾çš„è´Ÿé¢è¯ï¼ˆå¦‚"ä¼¤å¿ƒ"ã€"éš¾è¿‡"ï¼‰ï¼Œå…³é”®è¯æ–¹æ³•æ›´å¯é 
                if prediction_conflict and keyword_result["confidence"] > 0.7:
                    # å¦‚æœå…³é”®è¯é¢„æµ‹ç½®ä¿¡åº¦é«˜ä¸”ä¸æ¨¡å‹å†²çªï¼Œä¼˜å…ˆä½¿ç”¨å…³é”®è¯ç»“æœ
                    sentiment = keyword_result["sentiment"]
                    # é™ä½æ¨¡å‹æƒé‡ï¼Œæé«˜å…³é”®è¯æƒé‡
                    combined_confidence = (confidence * 0.3 + keyword_result["confidence"] * 0.7)
                    confidence = combined_confidence
                    return {
                        "sentiment": sentiment,
                        "confidence": float(confidence),
                        "score": sentiment_score,
                        "method": "model_keywords_combined",
                        "model_sentiment": "æ­£é¢" if sentiment_score >= 0.5 else "è´Ÿé¢",
                        "keyword_sentiment": keyword_result["sentiment"]
                    }
                else:
                    # æ¨¡å‹ä¸ç¡®å®šä½†æ— å†²çªï¼Œæˆ–å…³é”®è¯ä¹Ÿä¸ç¡®å®šï¼Œä½¿ç”¨åŠ æƒå¹³å‡
                    combined_confidence = (confidence * 0.4 + keyword_result["confidence"] * 0.6)
                    if abs(sentiment_score - 0.5) < 0.1:  # æ¥è¿‘ä¸­æ€§æ—¶ï¼Œå‚è€ƒå…³é”®è¯ç»“æœ
                        sentiment = keyword_result["sentiment"]
                    confidence = combined_confidence
            
            return {
                "sentiment": sentiment,
                "confidence": float(confidence),
                "score": sentiment_score,
                "method": "model"
            }
        except Exception as e:
            print(f"æ¨¡å‹é¢„æµ‹é”™è¯¯: {e}")
            # å‡ºé”™æ—¶å›é€€åˆ°å…³é”®è¯æ–¹æ³•
            result = self.predict_with_keywords(text)
            result["method"] = "keywords_error_fallback"
            return result

# å…¨å±€å®ä¾‹
sentiment_analyzer = SentimentAnalyzer()

ment_analysis.pyâ€¦]()

ç››æ‰åšï¼š
[Uploadin# æœºå™¨ç¿»è¯‘æ¨¡å‹åŠ è½½å’Œæ¨ç†
import os
import re
import numpy as np
import tensorflow as tf
import pickle
import sys

# é…ç½®TensorFlowä½¿ç”¨CPUï¼ˆé¿å…GPUç›¸å…³é”™è¯¯ï¼‰
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
tf.config.set_visible_devices([], 'GPU')

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import (
    TRANSLATE_CHECKPOINT_DIR,
    TRANSLATE_DATA_PATH,
    TRANSLATE_TOKENIZER_PATH,
    TRANSLATE_CONFIG_PATH
)


class Translator:
    def __init__(self):
        # ä¸­è¯‘è‹±æ¨¡å‹ï¼ˆä¸­æ–‡â†’è‹±æ–‡ï¼‰
        self.encoder_zh2en = None
        self.decoder_zh2en = None
        # è‹±è¯‘ä¸­æ¨¡å‹ï¼ˆè‹±æ–‡â†’ä¸­æ–‡ï¼‰
        self.encoder_en2zh = None
        self.decoder_en2zh = None
        
        self.inp_lang = None  # è¾“å…¥è¯­è¨€ï¼ˆä¸­æ–‡ï¼‰
        self.targ_lang = None  # ç›®æ ‡è¯­è¨€ï¼ˆè‹±æ–‡ï¼‰
        self.max_length_targ = None
        self.max_length_inp = None
        self.units = 1024
        self.embedding_dim = 256
        self.BATCH_SIZE = 1
        self.model_loaded_zh2en = False
        self.model_loaded_en2zh = False
        self.load_model()

    def preprocess_sentence(self, w):
        """é¢„å¤„ç†å¥å­ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´"""
        if not w:
            return ""
        w = str(w).strip()
        # å¯¹å¥å­ä¸­æ ‡ç‚¹ç¬¦å·å‰ååŠ ç©ºæ ¼
        w = re.sub(r'([?.!,])', r' \1 ', w)
        # å°†å¥å­ä¸­å¤šç©ºæ ¼å»é‡
        w = re.sub(r"[' ']+", ' ', w)
        # ç»™å¥å­åŠ ä¸Šå¼€å§‹å’Œç»“æŸæ ‡è®°
        w = '<start> ' + w.strip() + ' <end>'
        return w

    def load_model(self):
        """åŠ è½½æ¨¡å‹ï¼ˆç»“æ„ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰"""
        try:
            print(f"[DEBUG] Tokenizerè·¯å¾„: {TRANSLATE_TOKENIZER_PATH}")
            print(f"[DEBUG] Checkpointè·¯å¾„: {TRANSLATE_CHECKPOINT_DIR}")

            # 1. åŠ è½½Tokenizer
            if not os.path.exists(TRANSLATE_TOKENIZER_PATH):
                raise FileNotFoundError(f"Tokenizeræ–‡ä»¶ä¸å­˜åœ¨: {TRANSLATE_TOKENIZER_PATH}")

            with open(TRANSLATE_TOKENIZER_PATH, 'rb') as f:
                tokenizer_data = pickle.load(f)
                self.inp_lang = tokenizer_data['inp_lang']
                self.targ_lang = tokenizer_data['targ_lang']
                self.max_length_targ = tokenizer_data['max_length_targ']
                self.max_length_inp = tokenizer_data['max_length_inp']
                self.embedding_dim = tokenizer_data.get('embedding_dim', 256)
                self.units = tokenizer_data.get('units', 1024)

            # 2. æ£€æŸ¥Checkpointç›®å½•
            if not os.path.exists(TRANSLATE_CHECKPOINT_DIR):
                raise FileNotFoundError(f"Checkpointç›®å½•ä¸å­˜åœ¨: {TRANSLATE_CHECKPOINT_DIR}")

            # ===== æ¨¡å‹ç»“æ„å®šä¹‰ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰ =====
            class Encoder(tf.keras.Model):
                def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
                    super().__init__()
                    self.batch_sz = batch_sz
                    self.enc_units = enc_units
                    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)
                    self.bigru = tf.keras.layers.Bidirectional(
                        tf.keras.layers.GRU(
                            enc_units // 2,
                            return_sequences=True,
                            return_state=True,
                            recurrent_initializer='glorot_uniform',
                            dropout=0.2,
                            recurrent_dropout=0.2
                        )
                    )
                    self.state_proj = tf.keras.layers.Dense(enc_units, activation='tanh')

                def call(self, x, hidden, training=False):
                    x = self.embedding(x)
                    output, f_state, b_state = self.bigru(x, initial_state=[hidden, hidden], training=training)
                    state = self.state_proj(tf.concat([f_state, b_state], axis=-1))
                    return output, state

                def initialize_hidden_state(self):
                    return tf.zeros((self.batch_sz, self.enc_units // 2))

            class BahdanauAttention(tf.keras.layers.Layer):
                def __init__(self, units):
                    super().__init__()
                    self.W1 = tf.keras.layers.Dense(units)
                    self.W2 = tf.keras.layers.Dense(units)
                    self.V = tf.keras.layers.Dense(1)

                def call(self, query, values):
                    hidden_with_time_axis = tf.expand_dims(query, 1)
                    score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))
                    attention_weights = tf.nn.softmax(score, axis=1)
                    context_vector = attention_weights * values
                    context_vector = tf.reduce_sum(context_vector, axis=1)
                    return context_vector, attention_weights

            class Decoder(tf.keras.Model):
                def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
                    super().__init__()
                    self.batch_sz = batch_sz
                    self.dec_units = dec_units
                    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)
                    self.gru = tf.keras.layers.GRU(
                        dec_units,
                        return_sequences=True,
                        return_state=True,
                        recurrent_initializer='glorot_uniform',
                        dropout=0.2,
                        recurrent_dropout=0.2
                    )
                    self.fc_mid = tf.keras.layers.Dense(dec_units, activation='relu')
                    self.fc = tf.keras.layers.Dense(vocab_size)
                    self.attention = BahdanauAttention(dec_units)

                def call(self, x, hidden, enc_output, training=False):
                    context_vector, attention_weights = self.attention(hidden, enc_output)
                    x = self.embedding(x)
                    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
                    output, state = self.gru(x, initial_state=hidden, training=training)
                    output = tf.reshape(output, (-1, output.shape[2]))
                    output = self.fc_mid(output)
                    x = self.fc(output)
                    return x, state, attention_weights

            # ===== åˆ›å»ºä¸­è¯‘è‹±æ¨¡å‹å®ä¾‹ï¼ˆä¸­æ–‡â†’è‹±æ–‡ï¼‰ =====
            vocab_inp_size = len(self.inp_lang.word_index) + 1
            vocab_tar_size = len(self.targ_lang.word_index) + 1

            self.encoder_zh2en = Encoder(vocab_inp_size, self.embedding_dim, self.units, self.BATCH_SIZE)
            self.decoder_zh2en = Decoder(vocab_tar_size, self.embedding_dim, self.units, self.BATCH_SIZE)

            # ===== åˆ›å»ºè‹±è¯‘ä¸­æ¨¡å‹å®ä¾‹ï¼ˆè‹±æ–‡â†’ä¸­æ–‡ï¼Œäº¤æ¢vocabï¼‰ =====
            # æ³¨æ„ï¼šè‹±è¯‘ä¸­éœ€è¦äº¤æ¢è¾“å…¥è¾“å‡ºvocab
            self.encoder_en2zh = Encoder(vocab_tar_size, self.embedding_dim, self.units, self.BATCH_SIZE)
            self.decoder_en2zh = Decoder(vocab_inp_size, self.embedding_dim, self.units, self.BATCH_SIZE)

            # ===== åŠ è½½Checkpointï¼ˆä¸­è¯‘è‹±æ¨¡å‹ï¼‰ =====
            latest_checkpoint = tf.train.latest_checkpoint(TRANSLATE_CHECKPOINT_DIR)
            if not latest_checkpoint:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°checkpointæ–‡ä»¶: {TRANSLATE_CHECKPOINT_DIR}")

            # åŠ è½½ä¸­è¯‘è‹±æ¨¡å‹
            checkpoint_zh2en = tf.train.Checkpoint(encoder=self.encoder_zh2en, decoder=self.decoder_zh2en)
            status_zh2en = checkpoint_zh2en.restore(latest_checkpoint)
            status_zh2en.expect_partial()
            self.model_loaded_zh2en = True
            print(f"[INFO] ä¸­è¯‘è‹±æ¨¡å‹åŠ è½½æˆåŠŸ: {latest_checkpoint}")
            
            # å°è¯•åŠ è½½è‹±è¯‘ä¸­æ¨¡å‹
            # æ³¨æ„ï¼šç”±äºvocab sizeä¸åŒï¼Œæ— æ³•ç›´æ¥ä½¿ç”¨åŒä¸€ä¸ªcheckpoint
            # æˆ‘ä»¬å°è¯•åˆ›å»ºä¸€ä¸ªåå‘æ¨¡å‹ï¼Œä½†æƒé‡éœ€è¦é‡æ–°è®­ç»ƒæˆ–æ‰‹åŠ¨æ˜ å°„
            # è¿™é‡Œæˆ‘ä»¬å°è¯•åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡ï¼ˆæ•ˆæœè¾ƒå·®ï¼Œä½†å¯ä»¥è¿è¡Œï¼‰
            try:
                checkpoint_en2zh = tf.train.Checkpoint(
                    encoder=self.encoder_en2zh, 
                    decoder=self.decoder_en2zh
                )
                # å°è¯•ä»åŒä¸€ä¸ªcheckpointåŠ è½½ï¼ˆä¼šå¤±è´¥ï¼Œå› ä¸ºvocab sizeä¸åŒ¹é…ï¼‰
                # ä½†expect_partialä¼šå¿½ç•¥ä¸åŒ¹é…çš„éƒ¨åˆ†ï¼Œæ¨¡å‹ä¼šä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡
                status_en2zh = checkpoint_en2zh.restore(latest_checkpoint)
                status_en2zh.expect_partial()
                # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æƒé‡è¢«åŠ è½½
                # ç”±äºvocab sizeä¸åŒï¼Œembeddingå’Œfcå±‚æ— æ³•åŠ è½½ï¼Œä½†GRUç­‰å±‚å¯èƒ½å¯ä»¥å…±äº«
                # è¿™é‡Œæˆ‘ä»¬æ ‡è®°ä¸ºå·²åŠ è½½ï¼Œä½†å®é™…æ•ˆæœå¯èƒ½ä¸ç†æƒ³
                self.model_loaded_en2zh = True
                print(f"[INFO] è‹±è¯‘ä¸­æ¨¡å‹å·²åˆ›å»ºï¼ˆéƒ¨åˆ†æƒé‡å¯èƒ½æœªåŠ è½½ï¼Œæ•ˆæœå¯èƒ½ä¸ç†æƒ³ï¼‰")
                print(f"[INFO] å»ºè®®ï¼šå¦‚éœ€é«˜è´¨é‡è‹±è¯‘ä¸­ï¼Œè¯·è®­ç»ƒåå‘æ¨¡å‹")
            except Exception as e:
                print(f"[WARNING] è‹±è¯‘ä¸­æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
                print(f"[INFO] è‹±è¯‘ä¸­å°†ä½¿ç”¨ç®€åŒ–è¯å…¸ç¿»è¯‘")
                self.model_loaded_en2zh = False
            
            print(f"[INFO] æ¨¡å‹æ”¯æŒæ–¹å‘: ä¸­æ–‡ â†’ è‹±æ–‡ (zh2en): {'âœ“' if self.model_loaded_zh2en else 'âœ—'}")
            print(f"[INFO] æ¨¡å‹æ”¯æŒæ–¹å‘: è‹±æ–‡ â†’ ä¸­æ–‡ (en2zh): {'âœ“' if self.model_loaded_en2zh else 'âœ—'}")

        except Exception as e:
            print(f"[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.encoder_zh2en = None
            self.decoder_zh2en = None
            self.encoder_en2zh = None
            self.decoder_en2zh = None
            self.model_loaded_zh2en = False
            self.model_loaded_en2zh = False

    def evaluate(self, sentence, direction='zh2en'):
        """æ¨¡å‹æ¨ç†
        Args:
            sentence: å¾…ç¿»è¯‘çš„å¥å­
            direction: ç¿»è¯‘æ–¹å‘ï¼Œ'zh2en' è¡¨ç¤ºä¸­æ–‡â†’è‹±æ–‡ï¼Œ'en2zh' è¡¨ç¤ºè‹±æ–‡â†’ä¸­æ–‡
        """
        try:
            # æ ¹æ®æ–¹å‘é€‰æ‹©æ¨¡å‹å’Œè¯­è¨€
            if direction == 'zh2en':
                # ä¸­è¯‘è‹±ï¼šä½¿ç”¨ inp_langï¼ˆä¸­æ–‡ï¼‰ä½œä¸ºè¾“å…¥ï¼Œtarg_langï¼ˆè‹±æ–‡ï¼‰ä½œä¸ºè¾“å‡º
                if not self.model_loaded_zh2en or self.encoder_zh2en is None or self.decoder_zh2en is None:
                    return None
                
                encoder = self.encoder_zh2en
                decoder = self.decoder_zh2en
                input_lang = self.inp_lang
                output_lang = self.targ_lang
                max_input_len = self.max_length_inp
                max_output_len = self.max_length_targ
                
            elif direction == 'en2zh':
                # è‹±è¯‘ä¸­ï¼šä½¿ç”¨ targ_langï¼ˆè‹±æ–‡ï¼‰ä½œä¸ºè¾“å…¥ï¼Œinp_langï¼ˆä¸­æ–‡ï¼‰ä½œä¸ºè¾“å‡º
                if not self.model_loaded_en2zh or self.encoder_en2zh is None or self.decoder_en2zh is None:
                    return None
                
                encoder = self.encoder_en2zh
                decoder = self.decoder_en2zh
                input_lang = self.targ_lang  # è‹±æ–‡ä½œä¸ºè¾“å…¥
                output_lang = self.inp_lang  # ä¸­æ–‡ä½œä¸ºè¾“å‡º
                max_input_len = self.max_length_targ  # è‹±æ–‡çš„æœ€å¤§é•¿åº¦
                max_output_len = self.max_length_inp  # ä¸­æ–‡çš„æœ€å¤§é•¿åº¦
            else:
                return None

            sentence = self.preprocess_sentence(sentence)
            inputs = [input_lang.word_index.get(i, 0) for i in sentence.split() if i]
            if not inputs:
                return ""
            
            inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_input_len, padding='post')
            inputs = tf.convert_to_tensor(inputs)

            hidden = encoder.initialize_hidden_state()
            enc_out, enc_hidden = encoder(inputs, hidden, training=False)
            dec_hidden = enc_hidden

            start_token = output_lang.word_index['<start>'] if '<start>' in output_lang.word_index else 1
            dec_input = tf.expand_dims([start_token], 0)

            result = ""
            for _ in range(max_output_len):
                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out, training=False)
                predicted_id = tf.argmax(predictions[0]).numpy()
                predicted_word = output_lang.index_word.get(predicted_id, "")
                if predicted_word == '<end>':
                    break
                if predicted_word != '<start>':  # è·³è¿‡å¼€å§‹æ ‡è®°
                    result += predicted_word + " "
                dec_input = tf.expand_dims([predicted_id], 0)

            return result.strip()

        except Exception as e:
            print(f"[ERROR] æ¨ç†å¤±è´¥ ({direction}): {e}")
            import traceback
            traceback.print_exc()
            return None

    def simple_translate(self, text, direction='zh2en'):
        """å›ºå®šè¯å…¸ç¿»è¯‘ï¼ˆé™çº§ç”¨ï¼‰
        Args:
            text: å¾…ç¿»è¯‘çš„æ–‡æœ¬
            direction: ç¿»è¯‘æ–¹å‘ï¼Œ'zh2en' æˆ– 'en2zh'
        """
        if not text:
            return ""
        
        if direction == 'zh2en':
            # ä¸­è¯‘è‹±è¯å…¸ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çŸ­è¯­ï¼‰
            common_dict = {
                'å¾ˆé«˜å…´è§åˆ°ä½ ': 'Nice to meet you',
                'æ—©ä¸Šå¥½': 'Good morning', 
                'æ™šä¸Šå¥½': 'Good evening',
                'ä¸å®¢æ°”': "You're welcome",
                'æˆ‘çˆ±ä½ ': 'I love you',
                'ä½ å¥½': 'Hello', 
                'è°¢è°¢': 'Thank you', 
                'å†è§': 'Goodbye',
                'æ˜¯çš„': 'Yes', 
                'ä¸æ˜¯': 'No', 
                'å¯¹ä¸èµ·': 'Sorry',
                'è¯·': 'Please',
                'è°¢è°¢': 'Thanks',
                'å¥½çš„': 'OK',
                'æ²¡é—®é¢˜': 'No problem',
                'å½“ç„¶': 'Of course'
            }
            result = text
            # æŒ‰é•¿åº¦é™åºæ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çŸ­è¯­
            for zh, en in sorted(common_dict.items(), key=lambda x: len(x[0]), reverse=True):
                result = result.replace(zh, en)
            return result
        else:
            # è‹±è¯‘ä¸­è¯å…¸ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çŸ­è¯­ï¼‰
            common_dict = {
                'Nice to meet you': 'å¾ˆé«˜å…´è§åˆ°ä½ ',
                'Good morning': 'æ—©ä¸Šå¥½', 
                'Good evening': 'æ™šä¸Šå¥½',
                "You're welcome": 'ä¸å®¢æ°”',
                'I love you': 'æˆ‘çˆ±ä½ ',
                'Thank you': 'è°¢è°¢', 
                'Goodbye': 'å†è§',
                'Hello': 'ä½ å¥½',
                'Yes': 'æ˜¯çš„', 
                'No': 'ä¸æ˜¯', 
                'Sorry': 'å¯¹ä¸èµ·',
                'Please': 'è¯·',
                'Thanks': 'è°¢è°¢',
                'OK': 'å¥½çš„',
                'No problem': 'æ²¡é—®é¢˜',
                'Of course': 'å½“ç„¶'
            }
            result = text
            # æŒ‰é•¿åº¦é™åºæ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çŸ­è¯­
            for en, zh in sorted(common_dict.items(), key=lambda x: len(x[0]), reverse=True):
                result = result.replace(en, zh)
            return result

    def translate(self, text, direction='zh2en'):
        """å¯¹å¤–ç¿»è¯‘æ¥å£
        Args:
            text: å¾…ç¿»è¯‘çš„æ–‡æœ¬
            direction: ç¿»è¯‘æ–¹å‘ï¼Œ'zh2en' è¡¨ç¤ºä¸­æ–‡â†’è‹±æ–‡ï¼Œ'en2zh' è¡¨ç¤ºè‹±æ–‡â†’ä¸­æ–‡
        Returns:
            dict: åŒ…å« original, translated, method, direction çš„å­—å…¸
        """
        if direction not in ['zh2en', 'en2zh']:
            return {
                "original": text,
                "translated": "ä¸æ”¯æŒçš„æ–¹å‘ï¼Œè¯·ä½¿ç”¨ 'zh2en' æˆ– 'en2zh'",
                "method": "é”™è¯¯",
                "direction": direction
            }

        # ä¸­è¯‘è‹±ï¼šä½¿ç”¨æ¨¡å‹ç¿»è¯‘
        if direction == 'zh2en':
            if not self.model_loaded_zh2en:
                # æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨ç®€åŒ–ç¿»è¯‘
                return {
                    "original": text,
                    "translated": self.simple_translate(text, direction),
                    "method": "æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨ç®€åŒ–è¯å…¸ç¿»è¯‘",
                    "direction": direction
                }

            model_result = self.evaluate(text, direction)
            if model_result:
                return {
                    "original": text,
                    "translated": model_result,
                    "method": "Seq2Seqæ¨¡å‹ç¿»è¯‘",
                    "direction": direction
                }
            else:
                # æ¨¡å‹æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç¿»è¯‘
                return {
                    "original": text,
                    "translated": self.simple_translate(text, direction),
                    "method": "æ¨¡å‹æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–è¯å…¸ç¿»è¯‘",
                    "direction": direction
                }
        
        # è‹±è¯‘ä¸­ï¼šå°è¯•ä½¿ç”¨æ¨¡å‹ç¿»è¯‘ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç®€åŒ–ç¿»è¯‘
        else:  # direction == 'en2zh'
            # æ³¨æ„ï¼šç”±äºvocab sizeä¸åŒï¼Œè‹±è¯‘ä¸­æ¨¡å‹å¯èƒ½æ— æ³•ä»checkpointæ­£ç¡®åŠ è½½
            # å¦‚æœæ¨¡å‹å·²åŠ è½½ï¼Œå°è¯•ä½¿ç”¨ï¼›å¦åˆ™ç›´æ¥ä½¿ç”¨ç®€åŒ–ç¿»è¯‘
            if self.model_loaded_en2zh:
                model_result = self.evaluate(text, direction)
                if model_result and model_result.strip():
                    # æ£€æŸ¥ç»“æœæ˜¯å¦åˆç†ï¼ˆä¸æ˜¯ç©ºå­—ç¬¦ä¸²æˆ–åªæœ‰æ ‡ç‚¹ï¼‰
                    return {
                        "original": text,
                        "translated": model_result,
                        "method": "Seq2Seqæ¨¡å‹ç¿»è¯‘ï¼ˆéƒ¨åˆ†æƒé‡å¯èƒ½æœªåŠ è½½ï¼‰",
                        "direction": direction
                    }
            
            # æ¨¡å‹æœªåŠ è½½æˆ–æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç¿»è¯‘
            return {
                "original": text,
                "translated": self.simple_translate(text, direction),
                "method": "ç®€åŒ–è¯å…¸ç¿»è¯‘" + ("ï¼ˆæ¨¡å‹æƒé‡æœªæ­£ç¡®åŠ è½½ï¼‰" if self.model_loaded_en2zh else "ï¼ˆæ¨¡å‹æœªåŠ è½½ï¼‰"),
                "direction": direction,
                "note": "å¦‚éœ€é«˜è´¨é‡è‹±è¯‘ä¸­ï¼Œè¯·è®­ç»ƒåå‘æ¨¡å‹æˆ–ä½¿ç”¨ä¸“é—¨çš„è‹±è¯‘ä¸­checkpoint"
            }


# å…¨å±€å®ä¾‹
translator = Translator()g translator.pyâ€¦]()

# 10.4 ä»»åŠ¡ï¼šåŸºäºSeq2Seqçš„æœºå™¨ç¿»è¯‘
# ä»£ç 10-12 è¯­æ–™é¢„å¤„ç†
import re
import io
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os
import time
from tqdm import  tqdm
import numpy as np
# å‡†å¤‡æ•°æ®é›†
def preprocess_sentence(w):   
    '''
    wï¼šå¥å­
    '''
    w = re.sub(r'([?.!,])', r' \1 ', w)  # å¯¹å¥å­ä¸­æ ‡ç‚¹ç¬¦å·å‰ååŠ ç©ºæ ¼
    w = re.sub(r"[' ']+", ' ', w)  # å°†å¥å­ä¸­å¤šç©ºæ ¼å»é‡
    w = '<start> ' + w + ' <end>'  # ç»™å¥å­åŠ ä¸Šå¼€å§‹å’Œç»“æŸæ ‡è®°ï¼Œä»¥ä¾¿æ¨¡å‹é¢„æµ‹
    return w

en_sentence = 'I like this book'
sp_sentence = 'æˆ‘å–œæ¬¢è¿™æœ¬ä¹¦'
print('é¢„å¤„ç†å‰çš„è¾“å‡ºä¸ºï¼š', '\n', preprocess_sentence(en_sentence))
print('é¢„å¤„ç†å‰çš„è¾“å‡ºä¸ºï¼š', '\n', str(preprocess_sentence(sp_sentence)), 'utf-8', '\n')

# æ¸…ç†å¥å­ï¼Œåˆ é™¤é‡éŸ³ç¬¦å·ï¼Œè¿”å›æ ¼å¼ä¸º[è‹±æ–‡ï¼Œä¸­æ–‡]çš„å•è¯å¯¹
def create_dataset(path, num_examples):
    '''
    pathï¼šæ–‡ä»¶è·¯å¾„
    num_examplesï¼šé€‰ç”¨çš„æ•°æ®é‡
    '''
    lines = io.open(path, encoding='UTF-8').read().strip().split('\n')
    word_pairs = [[preprocess_sentence(w) for w in l.split('\t')]  for l in lines[:num_examples]]
    return zip(*word_pairs)

path_to_file = '/root/autodl-tmp/NLP/nlp_deeplearn/data/en-ch.txt'  # è¯»å–æ–‡ä»¶çš„è·¯å¾„
en, sp = create_dataset(path_to_file, None)  # æ•´åˆå¹¶è¯»å–æ•°æ®

# å¥å­çš„æœ€å¤§é•¿åº¦
def max_length(tensor):
    '''
    tensorï¼šæ–‡æœ¬æ„æˆçš„å¼ é‡
    '''
    return max(len(t) for t in tensor)

# tokenizeå‡½æ•°æ˜¯å¯¹æ–‡æœ¬ä¸­çš„è¯è¿›è¡Œç»Ÿè®¡è®¡æ•°ï¼Œç”Ÿæˆæ–‡æ¡£è¯å…¸ï¼Œä»¥æ”¯æŒåŸºäºè¯å…¸ä½åºç”Ÿæˆæ–‡æœ¬çš„å‘é‡è¡¨ç¤º
def tokenize(lang):
    '''
    langï¼šå¾…å¤„ç†çš„æ–‡æœ¬
    '''
    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
    lang_tokenizer.fit_on_texts(lang)
    tensor = lang_tokenizer.texts_to_sequences(lang)
    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')
    return tensor, lang_tokenizer

# åˆ›å»ºæ¸…ç†çš„è¾“å…¥è¾“å‡ºå¯¹
def load_dataset(path, num_examples=None):
    '''
    pathï¼šæ–‡ä»¶è·¯å¾„
    num_examplesï¼šé€‰ç”¨çš„æ•°æ®é‡
    '''
    # å»ºç«‹ç´¢å¼•ï¼Œå¹¶è¾“å…¥å·²ç»æ¸…æ´—è¿‡çš„è¯è¯­ï¼Œè¾“å‡ºè¯è¯­å¯¹
    targ_lang, inp_lang = create_dataset(path, num_examples) 
    # å»ºç«‹ä¸­æ–‡å¥å­çš„è¯å‘é‡ï¼Œå¯¹æ‰€æœ‰å¼ é‡è¿›è¡Œå¡«å……ï¼Œä½¿å¥å­çš„ç»´åº¦ä¸€æ ·
    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)   
    # å»ºç«‹è‹±æ–‡å¥å­çš„è¯å‘é‡ï¼Œå¯¹æ‰€æœ‰å¼ é‡è¿›è¡Œå¡«å……ï¼Œä½¿å¥å­çš„ç»´åº¦ä¸€æ ·
    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)  
    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer

num_examples = 2000  # è¯è¡¨çš„å¤§å°ï¼ˆè¯é‡ï¼‰
input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, 
                                                                num_examples)
# è®¡ç®—ç›®æ ‡å¼ é‡çš„æœ€å¤§é•¿åº¦ï¼ˆmax_lengthï¼‰
max_length_targ, max_length_inp = max_length(target_tensor), max_length(
    input_tensor) 

# é‡‡ç”¨8: 2çš„æ¯”ä¾‹åˆ‡åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(
        input_tensor, target_tensor, test_size=0.2) 

# éªŒè¯æ•°æ®æ­£ç¡®æ€§ï¼Œä¹Ÿå°±æ˜¯è¾“å‡ºè¯ä¸è¯è¯­æ˜ å°„ç´¢å¼•çš„è¡¨ç¤º
def convert(lang, tensor):
    '''
    langï¼šå¾…å¤„ç†çš„æ–‡æœ¬
    tensorï¼šæ–‡æœ¬æ„æˆçš„å¼ é‡
    '''
    for t in tensor:
        if t != 0:    
            print ('%d ----> %s' % (t, lang.index_word[t]))

print('é¢„å¤„ç†å‰çš„è¾“å‡ºä¸ºï¼š')
print('è¾“å…¥è¯­è¨€ï¼šè¯æ˜ å°„ç´¢å¼•')
convert(inp_lang, input_tensor_train[0])
print('ç›®æ ‡è¯­è¨€ï¼šè¯è¯­æ˜ å°„ç´¢å¼•')
convert(targ_lang, target_tensor_train[0])

# åˆ›å»ºtf.dataæ•°æ®é›†
BUFFER_SIZE = len(input_tensor_train)
BATCH_SIZE = 64          # å‡å° batchï¼Œæœ‰åˆ©äºæ”¶æ•›
steps_per_epoch = len(input_tensor_train)//BATCH_SIZE
embedding_dim = 256      # æé«˜è¯å‘é‡ç»´åº¦ï¼Œå¢å¼ºè¡¨è¾¾èƒ½åŠ›
units = 512              # ä¿æŒ 512ï¼Œé¿å…å¤ªæ…¢
vocab_inp_size = len(inp_lang.word_index)+1  # è¾“å…¥è¯è¡¨çš„å¤§å°
vocab_tar_size = len(targ_lang.word_index)+1  # è¾“å‡ºè¯è¡¨çš„å¤§å°
dataset = tf.data.Dataset.from_tensor_slices((
    input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)  # æ„å»ºè®­ç»ƒé›†
example_input_batch, example_target_batch = next(iter(dataset))



# ä»£ç 10-13 æ„å»ºæœºå™¨ç¿»è¯‘æ¨¡å‹
# åŒå‘ç¼–ç å™¨ï¼ˆBi-GRUï¼‰
class Encoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
        super(Encoder, self).__init__()
        self.batch_sz = batch_sz
        self.enc_units = enc_units
        # è¾“å…¥åµŒå…¥
        self.embedding = tf.keras.layers.Embedding(
            vocab_size, embedding_dim, mask_zero=True
        )
        # åŒå‘ GRU
        self.bigru = tf.keras.layers.Bidirectional(
            tf.keras.layers.GRU(
                self.enc_units,
                return_sequences=True,
                return_state=True,
                recurrent_initializer='glorot_uniform',
                dropout=0.2,
                recurrent_dropout=0.2
            )
        )
        # æŠŠå‰å‘/åå‘çŠ¶æ€æ‹¼æ¥åé™ç»´å› enc_units
        self.state_proj = tf.keras.layers.Dense(self.enc_units, activation='tanh')

    def call(self, x, hidden):
        x = self.embedding(x)
        # bigru è¿”å›ï¼šoutput, forward_state, backward_state
        output, f_state, b_state = self.bigru(x, initial_state=[hidden, hidden])
        # æ‹¼æ¥ä¸¤ä¸ªæ–¹å‘çš„ hiddenï¼Œå†æŠ•å½±å› enc_units
        h_cat = tf.concat([f_state, b_state], axis=-1)
        state = self.state_proj(h_cat)
        return output, state

    def initialize_hidden_state(self):
        return tf.zeros((self.batch_sz, self.enc_units))

# æ„å»ºç¼–ç å™¨ç½‘ç»œç»“æ„    
encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)  
print('ç¼–ç å™¨è¾“å‡ºå½¢çŠ¶ï¼š', '\n', ' (batch size, sequence length, units) {}'.format(sample_output.shape))
print('ç¼–ç å™¨éšè—çŠ¶æ€å½¢çŠ¶ï¼š', '\n', ' (batch size, units) {}'.format(sample_hidden.shape))

# æ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¿æŒåŸæ¥çš„ BahdanauAttention å®šä¹‰å³å¯ï¼‰
class BahdanauAttention(tf.keras.layers.Layer):
    def __init__(self, units):
        super(BahdanauAttention, self).__init__()
        self.W1 = tf.keras.layers.Dense(units, use_bias=False)
        self.W2 = tf.keras.layers.Dense(units, use_bias=False)
        self.V = tf.keras.layers.Dense(1)

    def call(self, query, values):
        hidden_with_time_axis = tf.expand_dims(query, 1)
        score = self.V(tf.nn.tanh(
            self.W1(values) + self.W2(hidden_with_time_axis)))
        attention_weights = tf.nn.softmax(score, axis=1)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        return context_vector, attention_weights

# è§£ç å™¨ï¼šå•å±‚ GRU + ä¸­é—´å…¨è¿æ¥
class Decoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
        super(Decoder, self).__init__()
        self.batch_sz = batch_sz
        self.dec_units = dec_units
        self.embedding = tf.keras.layers.Embedding(
            vocab_size, embedding_dim, mask_zero=True
        )
        self.gru = tf.keras.layers.GRU(
            self.dec_units,
            return_sequences=True,
            return_state=True,
            recurrent_initializer='glorot_uniform',
            dropout=0.2,
            recurrent_dropout=0.2
        )
        # æ–°å¢ä¸€ä¸ªä¸­é—´å…¨è¿æ¥å±‚
        self.fc_mid = tf.keras.layers.Dense(self.dec_units, activation='relu')
        self.fc = tf.keras.layers.Dense(vocab_size)
        self.attention = BahdanauAttention(self.dec_units)

    def call(self, x, hidden, enc_output):
        context_vector, attention_weights = self.attention(hidden, enc_output)
        x = self.embedding(x)
        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)
        output, state = self.gru(x, initial_state=hidden)
        output = tf.reshape(output, (-1, output.shape[2]))
        # å…ˆé€šè¿‡ä¸­é—´å±‚
        output = self.fc_mid(output)
        x = self.fc(output)
        return x, state, attention_weights

# æ„å»ºè§£ç å™¨ç½‘ç»œç»“æ„
decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)  
sample_decoder_output, states, attention_weight = decoder(
    tf.random.uniform((BATCH_SIZE, 1), maxval=vocab_tar_size, dtype=tf.int32),
    sample_hidden,
    sample_output
)
print('è§£ç å™¨è¾“å‡ºå½¢çŠ¶ï¼š', '\n', ' (batch_size, vocab size) {}'.format(sample_decoder_output.shape))


# ä»£ç 10-14 å®šä¹‰ä¼˜åŒ–å™¨åŠæŸå¤±å‡½æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
initial_learning_rate = 0.001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps=1000,
    decay_rate=0.96,
    staircase=True
)
optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

# å¸¦ label smoothing çš„æŸå¤±å‡½æ•°
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
def loss_function(real, pred):
    mask = tf.math.logical_not(tf.math.equal(real, 0))
    loss_ = loss_object(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_mean(loss_)



# ä»£ç 10-15 è®­ç»ƒæ¨¡å‹

# æ£€æŸ¥ç‚¹ï¼ˆåŸºäºå¯¹è±¡çš„ä¿å­˜ï¼‰ï¼Œå‡†å¤‡ä¿å­˜è®­ç»ƒæ¨¡å‹
checkpoint_dir = '/root/autodl-tmp/NLP/nlp_deeplearn/tmp/training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)  # ä¿å­˜æ¨¡å‹
# è®­ç»ƒæ¨¡å‹
def train(inp, targ, enc_hidden):
    '''
    inpï¼šæ‰¹æ¬¡
    targï¼šæ ‡ç­¾
    enc_hiddenï¼šéšè—æ ·æœ¬
    '''
    loss = 0
    with tf.GradientTape() as tape:
        enc_output, enc_hidden = encoder(inp, enc_hidden)  # æ„å»ºç¼–ç å™¨
        dec_hidden = enc_hidden  
        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)
        # æ•™å¸ˆå¼ºåˆ¶ - å°†ç›®æ ‡è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¾“å…¥
        for t in range(1, targ.shape[1]):
            # å°†ç¼–ç å™¨è¾“å‡ºä¼ é€è‡³è§£ç å™¨
            predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_output)
            loss += loss_function(targ[:, t], predictions)
            dec_input = tf.expand_dims(targ[:, t], 1)  # ä½¿ç”¨æ•™å¸ˆå¼ºåˆ¶
        loss = loss / int(targ.shape[1])  # è®¡ç®—å¹³å‡æŸå¤±
    batch_loss = loss.numpy()  # å°†æŸå¤±è½¬æ¢ä¸ºnumpyæ•°ç»„
    variables = encoder.trainable_variables + decoder.trainable_variables
    gradients = tape.gradient(loss, variables)
    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)
    optimizer.apply_gradients(zip(gradients, variables))

    return batch_loss

# å¼€å§‹è®­ç»ƒï¼ˆé€‚åº¦å¢åŠ è½®æ¬¡ä»¥æå‡å‡†ç¡®ç‡ï¼‰
EPOCHS = 30  # é€‚å½“å¢åŠ è®­ç»ƒè½®æ•°ï¼Œé€šå¸¸èƒ½æ˜æ˜¾æå‡ç¿»è¯‘è´¨é‡
loss = []

for epoch in tqdm(range(EPOCHS)):
    start = time.time()
    enc_hidden = encoder.initialize_hidden_state()  # åˆå§‹åŒ–éšè—å±‚
    total_loss = 0
    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
        batch_loss = train(inp, targ, enc_hidden)
        total_loss += batch_loss
        if batch % 50 == 0:  # å‡å°‘æ‰“å°é¢‘ç‡
            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))
            loss.append(round(batch_loss, 3))
    
    print('Epoch {} å¹³å‡æŸå¤±: {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))
    
    # æ¯5è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    if (epoch + 1) % 5 == 0:
        checkpoint.save(file_prefix=checkpoint_prefix)
        print('ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹')

# æŸå¤±è¶‹åŠ¿å¯è§†åŒ–

plt.rcParams['font.sans-serif'] = ['SIMHEI']  # è®¾ç½®å­—ä½“ä¸ºé»‘ä½“
plt.rcParams['axes.unicode_minus'] = False  # å¯¹å­—ç¬¦è¿›è¡Œæ˜¾ç¤ºè®¾ç½®
if loss:  # åªæœ‰å½“æœ‰æŸå¤±æ•°æ®æ—¶æ‰ç»˜å›¾
    plt.plot(list(range(1, len(loss)+1)), loss)  # å°†æŸå¤±å€¼ç»˜åˆ¶æˆæŠ˜çº¿å›¾
    plt.title('æŸå¤±è¶‹åŠ¿å›¾', fontsize=16)  # è®¾ç½®æŠ˜çº¿å›¾æ ‡é¢˜ä¸ºæŸå¤±è¶‹åŠ¿å›¾
    plt.xlabel('è¿­ä»£æ¬¡æ•°')  # å°†xè½´æ ‡ç­¾è®¾ç½®ä¸ºè¿­ä»£æ¬¡æ•°
    plt.ylabel('æŸå¤±å€¼')  # å°†yè½´æ ‡ç­¾è®¾ç½®ä¸ºæŸå¤±å€¼
    plt.show()  # å°†å›¾å½¢è¿›è¡Œå±•ç¤º
    plt.savefig("10_4.png")


# ä»£ç 10-16 ä½¿ç”¨æ¨¡å‹è¿›è¡Œè¯­å¥ç¿»è¯‘

# ä¼˜åŒ–çš„ç¿»è¯‘å‡½æ•°ï¼ˆæ”¯æŒbeam searchï¼‰
def evaluate(sentence, beam_width=1):
    '''
    sentenceï¼šéœ€è¦ç¿»è¯‘çš„å¥å­
    beam_widthï¼šbeam searchçš„å®½åº¦ï¼ˆ1è¡¨ç¤ºè´ªå¿ƒæœç´¢ï¼‰
    '''
    attention_plot = np.zeros((max_length_targ, max_length_inp))
    sentence = preprocess_sentence(sentence)
    inputs = [inp_lang.word_index.get(i, 0) for i in sentence.split(' ') if i in inp_lang.word_index]
    if not inputs:
        return '', sentence, attention_plot
    inputs = tf.keras.preprocessing.sequence.pad_sequences(
        [inputs], maxlen=max_length_inp, padding='post')
    inputs = tf.convert_to_tensor(inputs)
    result = ''
    hidden = tf.zeros((1, units))
    enc_out, enc_hidden = encoder(inputs, hidden)
    dec_hidden = enc_hidden
    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)
    
    if beam_width == 1:
        # è´ªå¿ƒæœç´¢
        for t in range(max_length_targ):
            predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)
            predicted_id = tf.argmax(predictions[0]).numpy()
            if predicted_id in targ_lang.index_word:
                predicted_word = targ_lang.index_word[predicted_id]
                if predicted_word == '<end>':
                    break
                result += predicted_word + ' '
            else:
                break
            dec_input = tf.expand_dims([predicted_id], 0)
    else:
        # ç®€åŒ–çš„beam searchï¼ˆå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
        for t in range(max_length_targ):
            predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)
            # è·å–top-ké¢„æµ‹
            top_k = tf.nn.top_k(predictions[0], k=min(beam_width, len(targ_lang.word_index)))
            predicted_id = top_k.indices[0].numpy()
            if predicted_id in targ_lang.index_word:
                predicted_word = targ_lang.index_word[predicted_id]
                if predicted_word == '<end>':
                    break
                result += predicted_word + ' '
            else:
                break
            dec_input = tf.expand_dims([predicted_id], 0)
    
    return result, sentence, attention_plot

# æ‰§è¡Œç¿»è¯‘â–²
def translate(sentence):
    '''
    sentenceï¼šè¦ç¿»è¯‘çš„å¥å­
    '''
    result, sentence, attention_plot = evaluate(sentence)
    print('è¾“å…¥ï¼š%s' % (sentence))
    print('ç¿»è¯‘ç»“æœï¼š{}'.format(result))

print(translate('æˆ‘ç”Ÿç—…äº†ã€‚'))
print(translate('ä¸ºä»€ä¹ˆä¸ï¼Ÿ'))
print(translate('è®©æˆ‘ä¸€ä¸ªäººå‘†ä¼šå„¿ã€‚'))
print(translate('æ‰“ç”µè¯å›å®¶ï¼'))
print(translate('æˆ‘äº†è§£ä½ ã€‚'))

# ===== æ–°å¢ï¼šä¿å­˜è®­ç»ƒç»“æœä»¥ä¾¿åœ¨qa_systemä¸­ä½¿ç”¨ =====
import pickle

# ä¿å­˜tokenizerå’Œæ¨¡å‹å‚æ•°
translate_save_dir = '/root/autodl-tmp/NLP/nlp_deeplearn/tmp/'
os.makedirs(translate_save_dir, exist_ok=True)

# ä¿å­˜tokenizer
tokenizer_save_path = os.path.join(translate_save_dir, 'translate_tokenizers.pkl')
with open(tokenizer_save_path, 'wb') as f:
    pickle.dump({
        'inp_lang': inp_lang,
        'targ_lang': targ_lang,
        'max_length_targ': max_length_targ,
        'max_length_inp': max_length_inp,
        'vocab_inp_size': vocab_inp_size,
        'vocab_tar_size': vocab_tar_size,
        'embedding_dim': embedding_dim,
        'units': units
    }, f)
print(f"\nç¿»è¯‘æ¨¡å‹tokenizerå·²ä¿å­˜åˆ°: {tokenizer_save_path}")

# ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯
config_save_path = os.path.join(translate_save_dir, 'translate_config.txt')
with open(config_save_path, 'w', encoding='utf-8') as f:
    f.write(f"max_length_targ={max_length_targ}\n")
    f.write(f"max_length_inp={max_length_inp}\n")
    f.write(f"vocab_inp_size={vocab_inp_size}\n")
    f.write(f"vocab_tar_size={vocab_tar_size}\n")
    f.write(f"embedding_dim={embedding_dim}\n")
    f.write(f"units={units}\n")
    f.write(f"checkpoint_dir={checkpoint_dir}\n")
print(f"ç¿»è¯‘æ¨¡å‹é…ç½®å·²ä¿å­˜åˆ°: {config_save_path}")

print("\nè®­ç»ƒç»“æœå·²ä¿å­˜ï¼Œç°åœ¨å¯ä»¥åœ¨qa_systemä¸­åŠ è½½ä½¿ç”¨äº†ã€‚")
print(f"æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
print(f"è¯·ç¡®ä¿åœ¨qa_systemä¸­ä½¿ç”¨æœ€æ–°çš„checkpointè¿›è¡ŒåŠ è½½ã€‚")

æ®·å°æ›¼ï¼š
# APIæ¨¡å—åˆå§‹åŒ–æ–‡ä»¶


# è±†åŒ… API é›†æˆ
import http.client
import json
from config import DOUBAO_API_URL, DOUBAO_API_KEY, DOUBAO_MODEL

class DoubaoAPI:
    def __init__(self):
        self.api_url = DOUBAO_API_URL
        self.api_key = DOUBAO_API_KEY
        self.model = DOUBAO_MODEL
        # ä»URLä¸­æå–ä¸»æœº
        if "https://" in self.api_url:
            self.host = self.api_url.replace("https://", "").split("/")[0]
            self.path = "/" + "/".join(self.api_url.replace("https://", "").split("/")[1:])
        else:
            self.host = "ark.cn-beijing.volces.com"
            self.path = "/api/v3/chat/completions"
    
    def chat(self, message, system_prompt="You are a helpful assistant.", conversation_history=None):
        """
        è°ƒç”¨è±†åŒ…APIè¿›è¡Œå¯¹è¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            conversation_history: å¯¹è¯å†å²è®°å½•
        """
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                {
                    "role": "system",
                    "content": system_prompt
                }
            ]
            
            # æ·»åŠ å†å²å¯¹è¯
            if conversation_history:
                messages.extend(conversation_history)
            
            # æ·»åŠ å½“å‰æ¶ˆæ¯
            messages.append({
                "role": "user",
                "content": message
            })
            
            # æ„å»ºè¯·æ±‚ä½“
            payload = json.dumps({
                "model": self.model,
                "messages": messages
            })
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                'Authorization': f'Bearer {self.api_key}',
                'Content-Type': 'application/json'
            }
            
            # å‘é€è¯·æ±‚
            conn = http.client.HTTPSConnection(self.host)
            conn.request("POST", self.path, payload, headers)
            res = conn.getresponse()
            data = res.read()
            conn.close()
            
            # è§£æå“åº”
            response_data = json.loads(data.decode("utf-8"))
            
            if "choices" in response_data and len(response_data["choices"]) > 0:
                content = response_data["choices"][0]["message"]["content"]
                return {
                    "success": True,
                    "content": content,
                    "full_response": response_data
                }
            else:
                return {
                    "success": False,
                    "error": "APIå“åº”æ ¼å¼é”™è¯¯",
                    "response": response_data
                }
        except Exception as e:
            return {
                "success": False,
                "error": f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
            }
    
    def ask(self, question):
        """ç®€å•é—®ç­”æ¥å£"""
        return self.chat(question)

# å…¨å±€å®ä¾‹
doubao_api = DoubaoAPI()

åˆ›æ„åŠŸèƒ½ï¼š
[__init__.py](https://github.com/user-attachments/files/24687188/__init__.py)
[creative_features.py](https://github.com/user-attachments/files/24687189/creative_features.py)

[index.html](https://github.com/user-attachments/files/24687191/index.html)
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ™ºèƒ½é—®ç­”ç³»ç»Ÿ</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body>
    <!-- èƒŒæ™¯è£…é¥°å…ƒç´  -->
    <div class="anime-decorations">
        <div class="floating-cat cat-1">ğŸ±</div>
        <div class="floating-cat cat-2">âœ¨</div>
        <div class="floating-cat cat-3">â­</div>
        <div class="floating-cat cat-4">ğŸ’«</div>
        <div class="floating-cat cat-5">ğŸŒŸ</div>
        <div class="floating-cat cat-6">ğŸ’–</div>
        <div class="floating-cat cat-7">ğŸ¾</div>
        <div class="floating-cat cat-8">ğŸŒ¸</div>
    </div>
    
    <div class="container">
        <header>
            <div class="header-content">
                <img src="{{ url_for('static', filename='images/cat-icon.svg') }}" alt="æ™ºèƒ½åŠ©æ‰‹" class="header-icon" onerror="this.style.display='none'; this.nextElementSibling.style.display='inline-block';">
                <span class="header-icon-fallback" style="display:none;">ğŸ±</span>
                <div class="header-text">
                    <h1>å¤šåŠŸèƒ½æ™ºèƒ½é—®ç­”ç³»ç»Ÿ</h1>
                    <p class="subtitle">é›†æˆè±†åŒ…APIã€æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ç­‰åŠŸèƒ½</p>
                </div>
            </div>
        </header>

        <div class="main-wrapper">
            <!-- å·¦ä¾§åŠŸèƒ½é€‰æ‹©æ ‡ç­¾ -->
            <div class="sidebar">
                <div class="tabs">
                    <button class="tab-btn active" data-tab="chat">ğŸ’¬ æ™ºèƒ½é—®ç­”</button>
                    <button class="tab-btn" data-tab="classify">ğŸ“ æ–‡æœ¬åˆ†ç±»</button>
                    <button class="tab-btn" data-tab="sentiment">ğŸ˜Š æƒ…æ„Ÿåˆ†æ</button>
                    <button class="tab-btn" data-tab="translate">ğŸŒ æœºå™¨ç¿»è¯‘</button>
                    <button class="tab-btn" data-tab="creative">âœ¨ åˆ›æ„åŠŸèƒ½</button>
                </div>
            </div>

            <!-- å³ä¾§ä¸»å†…å®¹åŒºåŸŸ -->
            <div class="main-content">
                <!-- æ™ºèƒ½é—®ç­”é¢æ¿ -->
            <div class="tab-content active" id="chat">
                <div class="chat-container">
                    <div class="chat-messages" id="chatMessages">
                        <div class="message system cat-message">
                            <div class="cat-ears">
                                <span class="ear-left">ğŸ±</span>
                                <span class="ear-right">ğŸ±</span>
                            </div>
                            <p>ğŸ‘‹ æ¬¢è¿ä½¿ç”¨å“ˆåŸºç±³æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼æˆ‘æ˜¯æ‚¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ã€‚è¯·éšæ—¶å‘æˆ‘æé—®ï¼</p>
                            <div class="cat-tail">ğŸ¾</div>
                        </div>
                    </div>
                    <div class="chat-input-area">
                        <div class="input-wrapper">
                            <span class="cat-emoji-input">ğŸ±</span>
                            <textarea id="chatInput" placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜..."></textarea>
                        </div>
                        <button id="sendBtn" class="btn-send">å‘é€ ğŸ¾</button>
                    </div>
                </div>
            </div>

            <!-- æ–‡æœ¬åˆ†ç±»é¢æ¿ -->
            <div class="tab-content" id="classify">
                <div class="feature-panel">
                    <h3>æ–‡æœ¬åˆ†ç±»</h3>
                    <p>å°†æ–‡æœ¬åˆ†ç±»åˆ°ä»¥ä¸‹ç±»åˆ«ï¼šä½“è‚²ã€è´¢ç»ã€æˆ¿äº§ã€å®¶å±…ã€æ•™è‚²ã€ç§‘æŠ€ã€æ—¶å°šã€æ—¶æ”¿ã€æ¸¸æˆã€å¨±ä¹</p>
                    <textarea id="classifyInput" placeholder="è¾“å…¥è¦åˆ†ç±»çš„æ–‡æœ¬..."></textarea>
                    <button class="btn-primary" onclick="classifyText()">åˆ†ç±»</button>
                    <div id="classifyResult" class="result-box"></div>
                </div>
            </div>

            <!-- æƒ…æ„Ÿåˆ†æé¢æ¿ -->
            <div class="tab-content" id="sentiment">
                <div class="feature-panel">
                    <h3>æƒ…æ„Ÿåˆ†æ</h3>
                    <p>åˆ†ææ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰</p>
                    <textarea id="sentimentInput" placeholder="è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬..."></textarea>
                    <button class="btn-primary" onclick="analyzeSentiment()">åˆ†æ</button>
                    <div id="sentimentResult" class="result-box"></div>
                </div>
            </div>

            <!-- æœºå™¨ç¿»è¯‘é¢æ¿ -->
            <div class="tab-content" id="translate">
                <div class="feature-panel">
                    <h3>æœºå™¨ç¿»è¯‘</h3>
                    <p>æ”¯æŒä¸­è‹±æ–‡äº’è¯‘</p>
                    <div class="translate-controls">
                        <label>
                            <input type="radio" name="direction" value="zh2en" checked> ä¸­æ–‡ â†’ è‹±æ–‡
                        </label>
                        <label>
                            <input type="radio" name="direction" value="en2zh"> è‹±æ–‡ â†’ ä¸­æ–‡
                        </label>
                    </div>
                    <textarea id="translateInput" placeholder="è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬..."></textarea>
                    <button class="btn-primary" onclick="translateText()">ç¿»è¯‘</button>
                    <div id="translateResult" class="result-box"></div>
                </div>
            </div>

            <!-- åˆ›æ„åŠŸèƒ½é¢æ¿ -->
            <div class="tab-content" id="creative">
                <div class="feature-panel">
                    <h3>åˆ›æ„åŠŸèƒ½</h3>
                    <div class="creative-buttons">
                        <button class="btn-secondary" onclick="extractKeywords()">ğŸ”‘ æå–å…³é”®è¯</button>
                        <button class="btn-secondary" onclick="generateSummary()">ğŸ“„ æ–‡æœ¬æ‘˜è¦</button>
                        <button class="btn-secondary" onclick="wordFrequency()">ğŸ“Š è¯é¢‘ç»Ÿè®¡</button>
                        <button class="btn-secondary" onclick="textStatistics()">ğŸ“ˆ æ–‡æœ¬ç»Ÿè®¡</button>
                        <button class="btn-secondary" onclick="detectLanguage()">ğŸŒ è¯­è¨€æ£€æµ‹</button>
                    </div>
                    <textarea id="creativeInput" placeholder="è¾“å…¥æ–‡æœ¬ä»¥ä½¿ç”¨åˆ›æ„åŠŸèƒ½..."></textarea>
                    <div id="creativeResult" class="result-box"></div>
                </div>
            </div>
            </div>
        </div>

        <footer>
            <p>ç³»ç»ŸçŠ¶æ€: <span id="systemStatus">æ£€æŸ¥ä¸­...</span></p>
        </footer>
    </div>

    <script src="{{ url_for('static', filename='js/main.js') }}"></script>
</body>
</html>


